{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP FIRST ASSIGNMENT\n",
    "Developing a Naïve Bayas Classifier able to distinguish between english and not-english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sebastianodarconso/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import random \n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "ROOT = '/Users/sebastianodarconso/Desktop/magistrale_lab/natural_language_processing/first_assingnment/europarl_raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting english and not-english files from the europarl_raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = open(ROOT + \"/english/ep-00-01-17.en\", 'r').read()\n",
    "eng += open(ROOT + \"/english/ep-00-01-18.en\", 'r').read()\n",
    "eng += open(ROOT + \"/english/ep-00-01-19.en\", 'r').read()\n",
    "eng += open(ROOT + \"/english/ep-00-01-21.en\", 'r').read()\n",
    "eng += open(ROOT + \"/english/ep-00-02-02.en\", 'r').read()\n",
    "eng += open(ROOT + \"/english/ep-00-02-03.en\", 'r').read()\n",
    "\n",
    "not_eng = open(ROOT + \"/german/ep-00-01-17.de\", 'r').read()\n",
    "not_eng += open(ROOT + \"/french/ep-00-01-17.fr\", 'r').read()\n",
    "not_eng += open(ROOT + \"/finnish/ep-00-01-17.fi\", 'r').read()\n",
    "not_eng += open(ROOT + \"/greek/ep-00-01-17.el\", 'r').read()\n",
    "not_eng += open(ROOT + \"/italian/ep-00-01-17.it\", 'r').read()\n",
    "not_eng += open(ROOT + \"/swedish/ep-00-01-17.sv\", 'r').read()\n",
    "not_eng += open(ROOT + \"/dutch/ep-00-01-18.nl\", 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400752\n",
      "1503692\n"
     ]
    }
   ],
   "source": [
    "print(len(eng))\n",
    "print(len(not_eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sets of stopwords for the languages used \n",
    "For this example the languages are:\n",
    "- english\n",
    "- german \n",
    "- dutch\n",
    "- finnish\n",
    "- italian\n",
    "- swedish \n",
    "- french \n",
    "- greek \n",
    "\n",
    "They will still be divided as \"english\" and \"not english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['english', 'german', 'dutch', 'finnish', 'italian', 'swedish', 'french', 'greek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_eng = set()\n",
    "stopwords_not_eng = set()\n",
    "for l in languages:\n",
    "    if l == 'english':\n",
    "        stopwords_eng.update(stopwords.words(l))\n",
    "    else:\n",
    "        stopwords_not_eng.update(stopwords.words(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the stemmer (PorterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing, stemming and removing stopwords\n",
    "\n",
    "In this section all the words in both english and not english bows will be tokenized and stemmed. From the resulting bows will be removed also the stopwords and then they will be merged together in order to create an heterogeneous (with relation to the language) bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9135/9135 [00:01<00:00, 4995.50it/s]\n",
      "100%|██████████| 9637/9637 [00:01<00:00, 5111.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7317121028900146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "start = time.time()\n",
    "for en in tqdm(eng.split('\\n')):\n",
    "    documents.append((en, 'eng'))\n",
    "    words = word_tokenize(en)\n",
    "    for w in words:\n",
    "        if not w in stopwords_eng:\n",
    "            all_words.append(ps.stem(w.lower()))\n",
    "\n",
    "for ne in tqdm(not_eng.split('\\n')):\n",
    "    documents.append((ne, 'not eng'))\n",
    "    words = word_tokenize(ne)\n",
    "    for w in words:\n",
    "        if w not in stopwords_not_eng:\n",
    "            all_words.append(ps.stem(w.lower()))\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the frequency distribution for the BOW and listing the first 8k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:8000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function that extract the features from each document (all the words, tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset and shuffling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m featureset \u001b[38;5;241m=\u001b[39m [(find_features(doc), lang) \u001b[38;5;28;01mfor\u001b[39;00m (doc, lang) \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m      2\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(featureset)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(featureset))\n",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m featureset \u001b[38;5;241m=\u001b[39m [(\u001b[43mfind_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m, lang) \u001b[38;5;28;01mfor\u001b[39;00m (doc, lang) \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m      2\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(featureset)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(featureset))\n",
      "Cell \u001b[0;32mIn [10], line 5\u001b[0m, in \u001b[0;36mfind_features\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m      3\u001b[0m features \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_features:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (w \u001b[38;5;129;01min\u001b[39;00m words)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "featureset = [(find_features(doc), lang) for (doc, lang) in tqdm(documents)]\n",
    "random.shuffle(featureset)\n",
    "print(len(featureset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting (evenly) the dataset into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = len(featureset)\n",
    "\n",
    "testing_set = featureset[(dataset_len//2):]\n",
    "training_set = featureset[:(dataset_len//2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the default Naïve Bayas Classifier on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3425772190094\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the function that will be used to test the classifier on text and files different from the testing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(text):\n",
    "    feats = find_features(text)\n",
    "    return classifier.classify(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oggi è proprio una bella giornata: not eng\n",
      "\n",
      "today is such a beautiful day: eng\n",
      "\n",
      "Mein Name ist Anna. Ich komme aus Österreich und lebe seit drei Jahren in Deutschland. Ich bin 15Jahre alt und habe zwei Geschwister: Meine Schwester heißt Klara und ist 13 Jahre alt, mein BruderMichael ist 18 Jahre alt.: not eng\n",
      "\n",
      "Το όνομά μου είναι Άννα. Κατάγομαι από την Αυστρία και ζω στη Γερμανία εδώ και τρία χρόνια. Είμαι 15 χρονών και έχω δύο αδέρφια: Η αδερφή μου ονομάζεται Κλάρα και είναι 13 ετών, ο αδερφός μου ο Μιχαήλ είναι 18 ετών. Ζούμε με τους γονείς μας σε ένα σπίτι κοντά στο Μόναχο, η μητέρα μου είναι μαγείρισσα και ο πατέρας μου σε τράπεζα: not eng\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"oggi è proprio una bella giornata\"\n",
    "text_eng = \"today is such a beautiful day\"\n",
    "text_de = \"Mein Name ist Anna. Ich komme aus Österreich und lebe seit drei Jahren in Deutschland. Ich bin 15Jahre alt und habe zwei Geschwister: Meine Schwester heißt Klara und ist 13 Jahre alt, mein BruderMichael ist 18 Jahre alt.\"\n",
    "text_el = \"Το όνομά μου είναι Άννα. Κατάγομαι από την Αυστρία και ζω στη Γερμανία εδώ και τρία χρόνια. Είμαι 15 χρονών και έχω δύο αδέρφια: Η αδερφή μου ονομάζεται Κλάρα και είναι 13 ετών, ο αδερφός μου ο Μιχαήλ είναι 18 ετών. Ζούμε με τους γονείς μας σε ένα σπίτι κοντά στο Μόναχο, η μητέρα μου είναι μαγείρισσα και ο πατέρας μου σε τράπεζα\"\n",
    "print(text + ': ' + test(text) + '\\n')\n",
    "print(text_eng + ': ' + test(text_eng) + '\\n')\n",
    "print(text_de + ': '+ test(text_de) + '\\n')\n",
    "print(text_el + ': '+ test(text_el) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the confusion matrix with the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        |         n |\n",
      "        |         o |\n",
      "        |         t |\n",
      "        |           |\n",
      "        |    e    e |\n",
      "        |    n    n |\n",
      "        |    g    g |\n",
      "--------+-----------+\n",
      "    eng |<4382> 173 |\n",
      "not eng |    .<4831>|\n",
      "--------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "from collections import defaultdict\n",
    "\n",
    "ref = defaultdict(set)\n",
    "testset = defaultdict(set)\n",
    "\n",
    "labels = []\n",
    "tests = []\n",
    "\n",
    "for i, (feats, label) in enumerate(tqdm(testing_set)):\n",
    "    ref[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    labels.append(label)\n",
    "    tests.append(observed)\n",
    "\n",
    "\n",
    "cm = ConfusionMatrix(labels, tests)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the precision, recall and F-measure on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Tag | Prec.  | Recall | F-measure\n",
      "--------+--------+--------+-----------\n",
      "    eng | 1.0000 | 0.9620 | 0.9806\n",
      "not eng | 0.9654 | 1.0000 | 0.9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cm.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier accuracy and most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     and = True              eng : not en =    872.8 : 1.0\n",
      "                      en = True           not en : eng    =    683.3 : 1.0\n",
      "                     not = True              eng : not en =    572.6 : 1.0\n",
      "                      be = True              eng : not en =    402.3 : 1.0\n",
      "                     die = True           not en : eng    =    379.6 : 1.0\n",
      "                      la = True           not en : eng    =    256.4 : 1.0\n",
      "                       l = True           not en : eng    =    191.6 : 1.0\n",
      "                      le = True           not en : eng    =    183.9 : 1.0\n",
      "                      or = True              eng : not en =    175.9 : 1.0\n",
      "                      de = True           not en : eng    =    140.6 : 1.0\n",
      "                     den = True           not en : eng    =    136.3 : 1.0\n",
      "                       d = True           not en : eng    =    102.6 : 1.0\n",
      "                       i = True           not en : eng    =     98.9 : 1.0\n",
      "                    will = True              eng : not en =     80.5 : 1.0\n",
      "                       e = True           not en : eng    =     71.5 : 1.0\n",
      "accuracy: 0.983592584700618\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)\n",
    "print(\"accuracy: {}\".format(nltk.classify.accuracy(classifier, training_set)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e1bd50c29ca9d85d25a0084185febf7020f5e8bd5da929d18cf95b16060538f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
